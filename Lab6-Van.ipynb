{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "target_classifier = 'PC'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(get_confusion_costTot, greater_is_better=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Research on Cost Matrix\n",
    "# http://www.ibm.com/support/knowledgecenter/SSEPGG_11.1.0/com.ibm.im.model.doc/c_cost_matrix.html\n",
    "\n",
    "cost_matrix = np.matrix([[0,1,2,3,4],\n",
    "[1,0,1,2,3],\n",
    "[3,1,0,1,2],\n",
    "[5,3,1,0,1],\n",
    "[7,5,2,1,0]])\n",
    "\n",
    "def get_confusion_costTot(confusion_matrix, cost_matrix):\n",
    "    score = np.sum(confusion_matrix*cost_matrix)\n",
    "    return score\n",
    "\n",
    "confusion_scorer = make_scorer(get_confusion_costTot, greater_is_better=False)\n",
    "confusion_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the labels we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "X = X/5\n",
    "num_folds = 10\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits= num_folds, random_state=None, shuffle=True)\n",
    "cv_object.split(X,y)\n",
    "\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "        # I will create new variables here so that it is more obvious what \n",
    "        # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "        # but it makes this code less readable)\n",
    "        X_train = (X[train_indices])\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "    #     print(X_train)\n",
    "    #     print(y_train)\n",
    "\n",
    "        X_test = (X[test_indices])\n",
    "        y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from statistics import mode\n",
    "\n",
    "class MyEnsemble():\n",
    "    \n",
    "    def __init__(self, c, num_c, max_s, v):\n",
    "        self.Ensemble = BaggingClassifier(base_estimator= c,\n",
    "                                    n_estimators = num_c,\n",
    "                                     max_samples = max_s,\n",
    "                                     verbose = v)\n",
    "    def predict(self, X):\n",
    "        return self.Ensemble.predict(X)\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        self.Ensemble.fit(X,y)\n",
    "        \n",
    "    def fit_random(self, X, y):\n",
    "        self.fit(X,y) # just to get the ensemble estimators initialized\n",
    "        for classifier in self.Ensemble.estimators_:\n",
    "            row_indexes = np.random.randint(X.shape[0], size=self.Ensemble.max_samples) # gets row indexes\n",
    "            column_indexes = np.random.randint(X.shape[1], size=math.sqrt(X.shape[1])) #gets column indexes\n",
    "            classifier.fit(X[np.ix_(row_indexes,column_indexes)], y[row_indexes])\n",
    "            \n",
    "    def predict_random(self, x):\n",
    "        predictions = []\n",
    "        for classifier in self.Ensemble.estimators_:\n",
    "            predictions.append[classifier.predict(x)]\n",
    "        return(mode(predictions))\n",
    "            \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.Ensemble.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qvtra\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "C:\\Users\\qvtra\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 98]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1806eb286148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m# print(y_hat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\qvtra\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\qvtra\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\qvtra\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 98]"
     ]
    }
   ],
   "source": [
    "num_instances = 10\n",
    "\n",
    "\n",
    "ensemble = MyEnsemble(clf, 10,y_train.shape[0],False)\n",
    "\n",
    "ensemble.fit(X_train,y_train)\n",
    "y_hat=ensemble.predict(X_test[1])\n",
    "# print(y_hat)\n",
    "print(mt.confusion_matrix(y_hat,y_test))\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00791057,  0.07464913,  0.08540451,  0.57106081,  0.26097498],\n",
       "       [ 0.17690908,  0.09388587,  0.47283788,  0.22607983,  0.03028734],\n",
       "       [ 0.03334851,  0.13876676,  0.10925232,  0.20273776,  0.51589465],\n",
       "       [ 0.0070164 ,  0.03832066,  0.09122405,  0.53251585,  0.33092305],\n",
       "       [ 0.11645371,  0.15774367,  0.19207562,  0.46976262,  0.06396439],\n",
       "       [ 0.05869263,  0.1480663 ,  0.07848616,  0.15553635,  0.55921856],\n",
       "       [ 0.02600128,  0.0852406 ,  0.48533033,  0.26535816,  0.13806963],\n",
       "       [ 0.07152794,  0.56511451,  0.14878331,  0.14993491,  0.06463934],\n",
       "       [ 0.00339992,  0.02214786,  0.05431817,  0.20191905,  0.71821501],\n",
       "       [ 0.31253056,  0.30031308,  0.31707738,  0.04562797,  0.02445102],\n",
       "       [ 0.18726383,  0.3530118 ,  0.32230404,  0.09990162,  0.03751871],\n",
       "       [ 0.05014356,  0.05317366,  0.28557197,  0.22075782,  0.39035299],\n",
       "       [ 0.1813485 ,  0.09721054,  0.24648868,  0.37570903,  0.09924324],\n",
       "       [ 0.06814493,  0.48527787,  0.32443705,  0.10675238,  0.01538776],\n",
       "       [ 0.30926007,  0.41354927,  0.20065217,  0.04724577,  0.02929271],\n",
       "       [ 0.00263137,  0.01600553,  0.09151755,  0.15954396,  0.73030159],\n",
       "       [ 0.27048377,  0.25005712,  0.32188977,  0.13078811,  0.02678123],\n",
       "       [ 0.08669612,  0.29327917,  0.36808232,  0.15751556,  0.09442683],\n",
       "       [ 0.05928037,  0.11189324,  0.49159489,  0.13548428,  0.20174722],\n",
       "       [ 0.50130306,  0.32327495,  0.13387487,  0.03612992,  0.00541719],\n",
       "       [ 0.01247761,  0.01986735,  0.11412058,  0.44612925,  0.40740521],\n",
       "       [ 0.04988427,  0.21535704,  0.37572248,  0.20220657,  0.15682963],\n",
       "       [ 0.34178084,  0.29595191,  0.13050431,  0.19753125,  0.0342317 ],\n",
       "       [ 0.10844578,  0.29991023,  0.26396505,  0.26176592,  0.06591302],\n",
       "       [ 0.04376172,  0.377148  ,  0.12159437,  0.13725573,  0.32024019],\n",
       "       [ 0.36207347,  0.27323539,  0.22839356,  0.12636105,  0.00993653],\n",
       "       [ 0.20534291,  0.25457688,  0.2298427 ,  0.28863567,  0.02160184],\n",
       "       [ 0.09114102,  0.22913257,  0.40508313,  0.23433972,  0.04030356],\n",
       "       [ 0.02164731,  0.07347125,  0.20071064,  0.30535378,  0.39881703],\n",
       "       [ 0.26888734,  0.56790367,  0.10515849,  0.04864874,  0.00940176],\n",
       "       [ 0.16060857,  0.13517055,  0.48536649,  0.1089443 ,  0.1099101 ],\n",
       "       [ 0.04618553,  0.06923501,  0.28020208,  0.35564985,  0.24872753],\n",
       "       [ 0.27949033,  0.3003044 ,  0.33872839,  0.0647537 ,  0.01672318],\n",
       "       [ 0.27590404,  0.22872799,  0.37031452,  0.11717691,  0.00787654],\n",
       "       [ 0.39820608,  0.25198207,  0.23657524,  0.08910582,  0.02413078],\n",
       "       [ 0.34350811,  0.50142717,  0.05725314,  0.07922464,  0.01858693],\n",
       "       [ 0.049739  ,  0.16364309,  0.42486074,  0.21194748,  0.14980969],\n",
       "       [ 0.2741934 ,  0.52976202,  0.16351044,  0.02929827,  0.00323586],\n",
       "       [ 0.11888336,  0.27262561,  0.31275952,  0.20001696,  0.09571455],\n",
       "       [ 0.09612708,  0.09531988,  0.49305828,  0.15201849,  0.16347627],\n",
       "       [ 0.04203681,  0.12274846,  0.20334833,  0.48248068,  0.14938572],\n",
       "       [ 0.34240749,  0.54304032,  0.07592016,  0.03466406,  0.00396798],\n",
       "       [ 0.28424797,  0.56284446,  0.11009156,  0.03454343,  0.00827258],\n",
       "       [ 0.00825634,  0.02221519,  0.26533067,  0.54178781,  0.16240999],\n",
       "       [ 0.30011558,  0.2588112 ,  0.21983742,  0.18626182,  0.03497397],\n",
       "       [ 0.01352359,  0.13087396,  0.32454121,  0.10973305,  0.4213282 ],\n",
       "       [ 0.19080597,  0.37466521,  0.16574668,  0.23171609,  0.03706606],\n",
       "       [ 0.12203278,  0.38016416,  0.36530849,  0.09201815,  0.04047642],\n",
       "       [ 0.00105583,  0.00398087,  0.0378773 ,  0.06840815,  0.88867784],\n",
       "       [ 0.01261123,  0.14504477,  0.07714485,  0.21330632,  0.55189284],\n",
       "       [ 0.2015254 ,  0.30388051,  0.2112595 ,  0.25802442,  0.02531016],\n",
       "       [ 0.0268196 ,  0.19746822,  0.31367035,  0.33914409,  0.12289773],\n",
       "       [ 0.09344543,  0.32371148,  0.38090028,  0.10447004,  0.09747277],\n",
       "       [ 0.15765619,  0.16515642,  0.45231839,  0.17313984,  0.05172917],\n",
       "       [ 0.05810575,  0.18143998,  0.35563057,  0.11038942,  0.29443428],\n",
       "       [ 0.15923024,  0.42331127,  0.31103048,  0.09282155,  0.01360646],\n",
       "       [ 0.10074503,  0.18247837,  0.37997159,  0.25981039,  0.07699463],\n",
       "       [ 0.00580407,  0.0155648 ,  0.16349573,  0.12804786,  0.68708754],\n",
       "       [ 0.21805835,  0.38231147,  0.26849557,  0.11754311,  0.0135915 ],\n",
       "       [ 0.068535  ,  0.16475735,  0.28706194,  0.10504736,  0.37459835],\n",
       "       [ 0.01655445,  0.03653984,  0.40448092,  0.29639882,  0.24602596],\n",
       "       [ 0.14595103,  0.25940266,  0.3145232 ,  0.17851889,  0.10160422],\n",
       "       [ 0.10091963,  0.04630598,  0.37700802,  0.31393497,  0.16183141],\n",
       "       [ 0.11710169,  0.27323232,  0.46198722,  0.10513297,  0.0425458 ],\n",
       "       [ 0.13056945,  0.07063458,  0.13149991,  0.46240176,  0.20489431],\n",
       "       [ 0.10790615,  0.24870633,  0.39682676,  0.15751308,  0.08904768],\n",
       "       [ 0.02300915,  0.05152862,  0.34501242,  0.4722934 ,  0.10815641],\n",
       "       [ 0.00366732,  0.01449688,  0.03611896,  0.12148858,  0.82422827],\n",
       "       [ 0.56787381,  0.278634  ,  0.08216562,  0.05716827,  0.01415829],\n",
       "       [ 0.10781801,  0.1545776 ,  0.47040752,  0.10524583,  0.16195104],\n",
       "       [ 0.16354463,  0.19085198,  0.35772419,  0.1818139 ,  0.10606529],\n",
       "       [ 0.02643718,  0.0489287 ,  0.15950641,  0.22474579,  0.54038192],\n",
       "       [ 0.026766  ,  0.06839805,  0.46531151,  0.22974842,  0.20977602],\n",
       "       [ 0.05855222,  0.10583034,  0.45086306,  0.22939931,  0.15535506],\n",
       "       [ 0.3828453 ,  0.3223987 ,  0.19217879,  0.08888425,  0.01369297],\n",
       "       [ 0.00949284,  0.03428324,  0.19348409,  0.21309598,  0.54964385],\n",
       "       [ 0.0546027 ,  0.15879576,  0.21156493,  0.18415937,  0.39087725],\n",
       "       [ 0.02104164,  0.20923192,  0.15151236,  0.19369382,  0.42452026],\n",
       "       [ 0.10845072,  0.22274887,  0.20132956,  0.27379342,  0.19367743],\n",
       "       [ 0.22995022,  0.51376589,  0.13069606,  0.07747986,  0.04810797],\n",
       "       [ 0.00651678,  0.02834726,  0.05410909,  0.34810191,  0.56292497],\n",
       "       [ 0.01723643,  0.03404365,  0.15884382,  0.39296739,  0.39690872],\n",
       "       [ 0.02187591,  0.07547637,  0.22412445,  0.50609372,  0.17242955],\n",
       "       [ 0.09363424,  0.29246054,  0.34323822,  0.19022619,  0.0804408 ],\n",
       "       [ 0.03553567,  0.07105294,  0.26208049,  0.30276298,  0.32856792],\n",
       "       [ 0.07502113,  0.23872996,  0.35020291,  0.19349139,  0.14255461],\n",
       "       [ 0.00757241,  0.0293592 ,  0.26003432,  0.2844108 ,  0.41862327],\n",
       "       [ 0.15951169,  0.4745776 ,  0.24129837,  0.05960198,  0.06501037],\n",
       "       [ 0.33139065,  0.31672627,  0.26298138,  0.07253978,  0.01636192],\n",
       "       [ 0.21839005,  0.45161086,  0.11734161,  0.18114791,  0.03150956],\n",
       "       [ 0.00800562,  0.07413418,  0.32178391,  0.21926232,  0.37681397],\n",
       "       [ 0.04630737,  0.13980325,  0.32748939,  0.20803458,  0.27836542],\n",
       "       [ 0.44399099,  0.47108518,  0.06852796,  0.01520597,  0.00118989],\n",
       "       [ 0.10259833,  0.31375898,  0.38818198,  0.1653113 ,  0.03014942],\n",
       "       [ 0.00730797,  0.01504138,  0.1331654 ,  0.28197538,  0.56250987],\n",
       "       [ 0.13043476,  0.29834903,  0.35826608,  0.16526672,  0.04768341],\n",
       "       [ 0.08850277,  0.24243033,  0.45671524,  0.11966712,  0.09268453],\n",
       "       [ 0.00147495,  0.00829134,  0.03407013,  0.26245938,  0.69370419]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " acc = mt.accuracy_score(y_test,y_hat)\n",
    "            #         lr_clf_accuracies.append(acc)\n",
    "            #         cost_accuracies.append([acc])\n",
    "\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "plot_confusion_matrix(conf, classes=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "plot_confusion_matrix(mt.confusion_matrix(y_test, y_hat), classes=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = get_confusion_costTot(conf, cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof showing dependence of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collecting the average base classifier error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.390816326531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sum_error = 0\n",
    "for classifiers in ensemble.Ensemble.estimators_:\n",
    "    y_hat = classifiers.predict(X_test)\n",
    "    sum_error += (1 - accuracy_score(y_test, y_hat))\n",
    "error_rate = sum_error/20\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collecting the ensemble error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62244897959183676"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.fit(X_train, y_train)\n",
    "y_hat = ensemble.predict(X_test)\n",
    "1 - accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collecting the theoretical ensemble error rate assuming classifiers are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "import math\n",
    "def ensemble_error(n_classifier, error):\n",
    "    k_start = math.ceil(n_classifier/2.0)\n",
    "    probs = [comb(n_classifier, k) *\n",
    "            error**k *\n",
    "            (1-error)**(n_classifier - k) for k in range (k_start, n_classifier + 1)]\n",
    "    return(sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21850639961887866"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_error(n_classifier=20, error=error_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of the theoretical ensemble error rate assuming classifiers are independent is significantly different than the probability of our actual ensemble error rate. So we know that the classifiers are not independent, meaning that the errors that the models make overlap.\n",
    "\n",
    "Logic as follows:\n",
    "\n",
    "#Assume classifiers are independent\n",
    "\n",
    "    if classifiers are independent, then ensemble error rate is k\n",
    "    \n",
    "    ensemble error rate is > k\n",
    "    \n",
    "    ensemble error rate is not k\n",
    "    \n",
    "    PROOF BY CONTRADICTION\n",
    "\n",
    "<b>Classifiers are not independent.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
